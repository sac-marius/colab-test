{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b9efff",
   "metadata": {},
   "source": [
    "# üöÄ Solana: Skalert uthenting av blokker med adaptiv parallellisme\n",
    "\n",
    "Denne notatboken henter blokker i batcher p√• 1000, og √∏ker antall samtidige tilkoblinger (`limit`) fra 20 opp til maks 50.\n",
    "\n",
    "‚úÖ Inkluderer:\n",
    "- Robust `asyncio` + `aiohttp`-pipeline\n",
    "- Automatisk retries og logging\n",
    "- Tidsm√•ling per batch\n",
    "- Slutter hvis feilmelding oppst√•r\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aiohttp pandas pyarrow tqdm nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab3a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4149fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RPC_URL = \"https://api.mainnet-beta.solana.com\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "async def fetch_block(session, slot, retries=3):\n",
    "    payload = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"id\": slot,\n",
    "        \"method\": \"getBlock\",\n",
    "        \"params\": [slot, {\"maxSupportedTransactionVersion\": 0}]\n",
    "    }\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with session.post(RPC_URL, headers=HEADERS, data=json.dumps(payload)) as resp:\n",
    "                if resp.status != 200:\n",
    "                    raise Exception(f\"Status {resp.status}\")\n",
    "                res = await resp.json()\n",
    "                result = res.get(\"result\", None)\n",
    "                if not result:\n",
    "                    return []\n",
    "\n",
    "                block_time = result.get(\"blockTime\")\n",
    "                txs = result.get(\"transactions\", [])\n",
    "                parsed = []\n",
    "                for tx in txs:\n",
    "                    message = tx['transaction']['message']\n",
    "                    meta = tx['meta']\n",
    "                    if not meta or not meta.get('postBalances'):\n",
    "                        continue\n",
    "\n",
    "                    accounts = message['accountKeys']\n",
    "                    from_addr = accounts[0]\n",
    "                    to_addr = accounts[1] if len(accounts) > 1 else None\n",
    "\n",
    "                    lamports = (\n",
    "                        meta['postBalances'][1] - meta['preBalances'][1]\n",
    "                        if len(meta['postBalances']) > 1 else 0\n",
    "                    )\n",
    "\n",
    "                    parsed.append({\n",
    "                        'slot': slot,\n",
    "                        'timestamp': block_time,\n",
    "                        'tx_signature': tx['transaction']['signatures'][0],\n",
    "                        'from_address': from_addr,\n",
    "                        'to_address': to_addr,\n",
    "                        'lamports': lamports\n",
    "                    })\n",
    "                return parsed\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\"‚ùå Feil etter {retries} fors√∏k p√• slot {slot}: {e}\")\n",
    "                return []\n",
    "            await asyncio.sleep(1)  # vent litt f√∏r retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8266b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    start_slot = 357230000  # Juster startpunkt\n",
    "    total_batches = 10      # Antall 1000-blokkers batcher (juster etter behov)\n",
    "    batch_size = 1000\n",
    "    limit = 20              # Startverdi for parallelle kall\n",
    "\n",
    "    all_results = []\n",
    "    for batch_num in range(total_batches):\n",
    "        current_slots = list(range(start_slot + batch_num * batch_size,\n",
    "                                   start_slot + (batch_num + 1) * batch_size))\n",
    "        print(f\"üì¶ Henter batch {batch_num+1} med limit={limit} for slots {current_slots[0]}‚Äì{current_slots[-1]}\")\n",
    "\n",
    "        batch_start = time.time()\n",
    "        connector = aiohttp.TCPConnector(limit=limit)\n",
    "        try:\n",
    "            async with aiohttp.ClientSession(connector=connector) as session:\n",
    "                tasks = [fetch_block(session, slot) for slot in current_slots]\n",
    "                batch_results = []\n",
    "                for future in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=f\"Batch {batch_num+1}\"):\n",
    "                    data = await future\n",
    "                    batch_results.extend(data)\n",
    "\n",
    "            df_batch = pd.DataFrame(batch_results)\n",
    "            df_batch['timestamp'] = pd.to_datetime(df_batch['timestamp'], unit='s')\n",
    "            df_batch.to_parquet(f\"solana_batch_{batch_num+1}.parquet\", index=False, compression='snappy')\n",
    "            print(f\"‚úÖ Batch {batch_num+1} ferdig p√• {time.time() - batch_start:.2f} sekunder. Lagret {len(df_batch)} rader.\")\n",
    "            all_results.extend(batch_results)\n",
    "\n",
    "            if limit < 50:\n",
    "                limit += 1  # √òk limit forsiktig\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"üõë Stoppet p√• batch {batch_num+1} med limit={limit}: {e}\")\n",
    "            break\n",
    "\n",
    "    print(f\"üéâ Ferdig. Totalt {len(all_results)} transaksjoner hentet.\")\n",
    "    df_all = pd.DataFrame(all_results)\n",
    "    df_all['timestamp'] = pd.to_datetime(df_all['timestamp'], unit='s')\n",
    "    df_all.to_parquet(\"solana_all_batches.parquet\", index=False, compression='snappy')\n",
    "\n",
    "await main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}